{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10353174-f58e-4885-aba2-00c9b5b0ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e885c43c",
   "metadata": {},
   "source": [
    "# Final project: Scikit-learn Pipeline example (with titanic data)\n",
    "(source: https://github.com/scikit-learn/blog/blob/main/assets/notebooks/sklearn-pandas-df-output.ipynb)\n",
    "\n",
    "This is our final project. The original pipeline demo is from a scikit-learn demo using the titanic data set. However, we would like to test it with our hypothesis-generated data frame. As you may guess, the data frame would be mimicking the titanic data. The advantage of using generated mimic data is that with the same set of data (in this case, titanic data) it is hard to spot any edge case and we cannot guarantee it will also work if there are new data coming in (not in this case for the titanic data but generally speaking).\n",
    "\n",
    "First, we will look at the orignal scikit-learn demo to understand the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644cc7e7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035d647",
   "metadata": {},
   "source": [
    "# Example: titanic dataset (with a Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd241d45-f95f-49d5-8b66-17adf66160d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff9ffc1-daa0-4de1-b494-a84a3b12816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9f7e6-d903-42d0-a56e-4c514c83a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d3546-051b-4eca-bdee-3aa885c0302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Here we use `StandardScaler` for continuous variables; \n",
    "#      then we impute for missing data (check the documentation for the imputation method)\n",
    "# We use `OneHotEncoder` for categorical variables\n",
    "# NOTE: we are using a subset of the features (not all the columns)\n",
    "\n",
    "ct = make_column_transformer((make_pipeline(SimpleImputer(), \n",
    "                                            StandardScaler()), [\"age\", \"fare\"]),\n",
    "                             (OneHotEncoder(sparse=False), [\"embarked\", \"sex\", \"pclass\"]), \n",
    "                             verbose_feature_names_out=False)\n",
    "\n",
    "# Note: click on pipeline elements to see more details\n",
    "clf = make_pipeline(ct, LogisticRegression())\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a88f293-9019-4915-b526-f7a1f871c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b42bd-c84a-4348-9ecb-b261909dca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove the last step in the pipeline (which is LogisticRegression()) & transform the X_test data\n",
    "\n",
    "clf[:-1].transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e8c2b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91f797-c5f9-4ee2-80de-787e98ee8b8a",
   "metadata": {},
   "source": [
    "Ok, now we want to test the pipeline `clf` (without the `LogisticRegression()`) with our generated data frame. It seems a lot, but here are some hints and steps you can consider when approaching it.\n",
    "\n",
    "1. Consider what values make sense in some columns, for categorical columns, you may want to inspect the titanic data frame first.\n",
    "2. Use `st.builds` to help generate categorical columns.\n",
    "3. Don't worry too much about the free text columns like `name` and `home.dest` - they are not being transformed anyway so we do not care if they are just random text\n",
    "4. Thinking of how you can test the transformed data frame (output) is what we are expecting the transformer to do.\n",
    "5. You may need to write multiple tests to make sure the output is what is expected\n",
    "\n",
    "Now the floor is yours. There will be no more hand-holding from this point on. But feel free to ask questions or work in groups if you found it easier. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411e08b0-876b-4392-973f-061bc5dec42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
